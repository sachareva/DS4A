# -*- coding: utf-8 -*-
"""In-Depth EDA Covid Data Cleanup.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-ErFsZSL7KihSk85tIYQ6Hk5JXyqnn9H

#COVID Data By County
##By Datasets
JHU Dataset

• No NaN in any column

• Will keep Countyname, ST_Name to join with Zip Codes. Also relevant are Fatality Rates, Unemployment Rates, Medium Household Income, Number of residents of each age range, and Racial Demographic Populations 

Key Takeaways: There is a strong correlation between black, hispanic, and aged population proportion with fatality. As each increases fatality rates also increase. We see that this is especially true for the outliers in fatality rates, most of which were concentrated around 2% but tended to correlate with higher Hispanic or Black population proportions. We also see that unemployment and median household income surprisingly don't have much correlation with fatality rates from our correlation plot.

DC Neighborhood Dataset

• No nulls in any column

• Will keep Date, Neighborhood, and Total Positives

Key Takeaways: There is a stead upward trend in positive cases that has been rapidly increasing in the past two months in the DC area, meaning there are variables we should investigate to see why neighborhoods in DC overall are getting more COVID cases more rapidly. There was a rapid spike or dip in individual neighborhood case numbers around September, which suggests that there might have been a lot of moving around or some kind of policy change around this time that affected the rates — or there is something strange with the records of this data as we don't see this spike in the overall trend.
"""

import json
import requests
from bs4 import BeautifulSoup
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy

"""##JHU USA County COVID Data"""

from google.colab import files
uploaded = files.upload()

import io
covdata = pd.read_csv(io.BytesIO(uploaded['covdatafinal.csv']))

#Get variable names
covdata.columns

covdata = covdata.drop(['OBJECTID', 'ST_Abbr', 'ST_ID', 'FIPS', 'Confirmedb', 'DeathsbyPo', 'PCTPOVALL_', 'Med_HH_Inc', 'State_Fata', 'DateChecke', 'url', 'Thumbnail', 'Unemployed', 'Median_Hou', 'Recovered', 'Active', 'State_Conf','State_Deat', 'State_Reco', 'State_Test', 'NewCases','NewDeaths', 'Wh_Alone','Bk_Alone', 'AI_Alone', 'As_Alone', 'NH_Alone', 'Agetotal', 'SO_Alone', 'Two_More', 'Day_1', 'Day_2','Day_3', 'Day_4', 'Day_5', 'Day_6', 'Day_7', 'Day_8', 'Day_9', 'Day_10','Day_11', 'Day_12', 'Day_13', 'Day_14', 'NewCasebyP', 'Shape__Area','Shape__Length'], axis=1)

#Check date types; not relevant as all data was collected on December 1st
covdata.dtypes

covdata['Countyname'] = covdata['Countyname'].astype(str)
covdata['ST_Name'] = covdata['ST_Name'].astype(str)
covdata['EM_type'] = covdata['EM_type'].astype(str)
covdata['EM_notes'] = covdata['EM_notes'].astype(str)
covdata['EM_date'] = covdata['EM_date'].astype(str)
#covdata['EM_date'] = pd.to_datetime(covdata['EM_date'], format= '%m/%d/%Y %I:%M:%S %p')

array = ['District of Columbia', 'Maryland', 'Virginia']
covdata = covdata.loc[covdata['ST_Name'].isin(array)]
array = ['District of Columbia', 'Alexandria', 'Arlington', 'Fairfax', 'Falls Church', 'Montgomery', 'Prince George']
covdata = covdata.loc[covdata['Countyname'].isin(array)]
covdata

#Check for NaNs
covdata.isnull().sum(axis=0).tolist()
#Since only 1 NaN exists and it's for an unassigned county, I deleted it.
covdata = covdata.dropna()
#Since we haven't finalized the list of counties/zip codes that will be included in our final analysis, I will use EDA for all Maryland/Virginia/DC counties and we can replicate this code for when we finalize the county list

#One Hot Encode Everything
y = pd.get_dummies(covdata.EM_type, prefix = 'EM_type')
y.head()
z = pd.get_dummies(covdata.EM_notes, prefix = 'EM_notes')
z.head()

covdata = pd.concat([covdata,y],axis=1)
covdata = pd.concat([covdata,z],axis=1)
covdata = covdata.drop(['EM_type', 'EM_notes'], axis = 1)

#Make upper case
covdata['Countyname'] = covdata['Countyname'].str.upper() 
covdata['ST_Name'] = covdata['ST_Name'].str.upper() 
covdata.to_csv(r'C:\Users\Minjue\Desktop\covdataAll.csv', index = False, header=True)

# Total number of rows and columns
covdata.shape

# Rows containing duplicate data
duplicate_rows_covdata = covdata[covdata.duplicated()]
#No duplicate rows
print('number of duplicate rows: ', duplicate_rows_covdata.shape)

#Get outliers for Fatality Rate
sns.boxplot(x=covdata['FatalityRa'])

# Plotting Unemployment Rate
covdata.Unemployme.value_counts().plot(kind='bar', figsize=(10,5))
plt.title('Unemployment Rate')
plt.ylabel('Number of Counties')
plt.xlabel('Unemployment Rate Percentage');

plt.figure(figsize=(20,10))
c= covdata.corr()
sns.heatmap(c,cmap= 'BrBG',annot=True)
c
#We see from this that there is strong correlation between fatality and black/hispanic population proportion, as well as aged population

# Plotting black population proportion against fatality rate
fig, ax = plt.subplots(figsize=(10,6))
ax.scatter(covdata['BlackPop']/covdata['TotalPop'], covdata['FatalityRa'])
ax.set_xlabel('Black Population Proportion')
ax.set_ylabel('Fatality Rate')
plt.show()

# Plotting hispanic population proportion against fatality rate
fig, ax = plt.subplots(figsize=(10,6))
ax.scatter(covdata['HispPop']/covdata['TotalPop'], covdata['FatalityRa'])
ax.set_xlabel('Hispanic Population')
ax.set_ylabel('Fatality Rate')
plt.show()

# Plotting aged population proportion against fatality rate
fig, ax = plt.subplots(figsize=(10,6))
ax.scatter(covdata['AgedPop']/covdata['TotalPop'], covdata['FatalityRa'])
ax.set_xlabel('Aged Population Proportion')
ax.set_ylabel('Fatality Rate')
plt.show()

"""##DC Covid Data"""

from google.colab import files
uploaded = files.upload()

dccovid = pd.read_csv(io.BytesIO(uploaded['dccovid.csv']))
dccovid.columns

dccovid.dtypes

dccovid['Date']= pd.to_datetime(dccovid['Date'])

dccovid.isnull().sum(axis=0).tolist()

result = dccovid.groupby(by=['Date']).sum().reset_index()
result.plot("Date", "Total Positives")

g = sns.FacetGrid(dccovid, col='Neighborhood')
g.map(plt.plot, 'Date', 'Total Positives')
plt.xticks(rotation=90)
plt.show()

dccovid

dccovid_clean = dccovid.rename(columns={'Total Positives': 'Confirmed'})
dccovid_clean

dccovid_clean.to_csv(r'Clean_DC_Covid_Neighborhood.csv', index = False, header=True)

from google.colab import files
uploaded = files.upload()

nbh_zip = pd.read_excel(io.BytesIO(uploaded['NBH_ZIP.xls']))

nbh_zip.dtypes

dcfcovid = pd.merge(dccovid_clean, nbh_zip,
                        how="left", on=["Neighborhood"])

dcfcovid

dcfcovid.isnull().sum(axis=0).tolist()
#all nulls are from Neighborhoods that are marked as "unknown"
dcfcovid = dcfcovid.dropna()
#drop all nulls since the unknown neighborhoods have no identifiable zip codes

dcfcovid.to_csv(r'Clean_DC_Covid_Neighborhood.csv', index = False, header=True)

from google.colab import files
uploaded = files.upload()

vdhcovid = pd.read_csv(io.BytesIO(uploaded['VDHCOVID.csv']))

vdhcovid = vdhcovid.drop(['Number of Testing Encounters', 'Number of PCR Testing Encounters'], axis = 1)

vdhcovid['Report Date']= pd.to_datetime(vdhcovid['Report Date'])
vdhcovid = vdhcovid.rename(columns={'Report Date': 'Date'})
vdhcovid = vdhcovid.rename(columns={'Number of Cases': 'Confirmed'})
vdhcovid["ST_Name"] = "VIRGINIA"
vdhcovid = vdhcovid.rename(columns={'ZIP Code': 'ZIP_CODE'})

vdhcovid.dtypes

vdhcovid['Confirmed'] = pd.to_numeric(vdhcovid['Confirmed'], errors='coerce').convert_dtypes() 
vdhcovid['ZIP_CODE'] = pd.to_numeric(vdhcovid['ZIP_CODE'], errors='coerce').convert_dtypes()

#Get number of suppressed data (now converted to nulls)
vdhcovid = vdhcovid.dropna(subset=['ZIP_CODE'])
vdhcovid.isnull().sum(axis=0).tolist()

vdhcovid

vdarray = [22301, 22302, 22303, 22304, 22305, 22306, 22307, 22308, 22309, 22310, 22311, 22312, 22314, 22315, 22331, 22332, 22202, 22201, 22204, 22203, 22206, 22205, 22209, 22207, 22213, 22212, 22217, 22225, 22227, 22230, 22030, 22031, 22032, 22003, 22015, 22027, 22031, 22030, 22033, 22032, 22035, 22039, 22041, 22043, 22042, 22046, 22044, 22060, 22066, 22079, 22082, 22101, 22102, 22122, 22124, 22151, 22150, 22153, 22152, 22181, 22180, 22182, 22303, 22307, 22306, 22309, 22308, 22311, 22310, 22312, 22315, 20120, 20121, 20124, 20151, 20171, 20170, 20191, 20190, 20192, 20194, 22040, 22042, 22044, 22046, 22003, 22015, 22027, 22031, 22030, 22033, 22032, 22035, 22039, 22041, 22043, 22042, 22046, 22044, 22060, 22066, 22079, 22082, 22101, 22102, 22122, 22124, 22151, 22150, 22153, 22152, 22181, 22180, 22182, 22303, 22307, 22306, 22309, 22308, 22311, 22310, 22312, 22315, 20120, 20121, 20124, 20151, 20171, 20170, 20191, 20190, 20192, 20194]
vdhcovid = vdhcovid.loc[vdhcovid['ZIP_CODE'].isin(vdarray)]
vdhcovid

vdhcovid.to_csv(r'Clean_VA_COVID_DATA.csv', index = False, header=True)

from google.colab import files
uploaded = files.upload()

mdcovid = pd.read_csv(io.BytesIO(uploaded['mdcovid.csv']))
mdcovid['Date']= pd.to_datetime(mdcovid['Date'])

mdcovid.dtypes

mdcovid = mdcovid[numpy.isin(mdcovid, [20810, 20812, 2081, 20814, 20816, 20815, 20818, 20817, 20825, 20824, 20830, 20827, 20833, 20832, 20838, 20837, 20841, 20839, 20847, 20842, 20849, 20848, 20851, 20850, 20853, 20852, 20855, 20854, 20859, 20857, 20861, 20860, 20866, 20862, 20871, 20868, 20874, 20872, 20876, 20875, 20878, 20877, 20880, 20879, 20883, 20058, 20882, 20885, 20884, 20886, 20895, 20896, 20899, 20898, 20902, 20901, 20904, 20903, 20906, 20905, 20907, 20910, 20912, 20914, 20916, 20703, 20705, 20704, 20707, 20706, 20709, 20708, 20710, 20712, 20716, 20715, 20718, 20717, 20720, 20719, 20722, 20721, 20725, 20731, 20735, 20737, 20740, 20738, 20742, 20741, 20744, 20743, 20746, 20745, 20748, 20747, 20750, 20749, 20752, 20753, 20757, 20762, 20769, 20768, 20771, 20770, 20773, 20772, 20775, 20774, 20782, 20781, 20784, 20783, 20785, 20792, 20791, 20607, 20608, 20613, 20623]).any(axis=1)]
mdcovid = mdcovid.rename(columns={'Delay': 'Confirmed'})
mdcovid = mdcovid.drop("OBJECTID", axis = 1)
mdcovid["ST_Name"] = "MARYLAND"

mdcovid

dcfcovid["ST_Name"] = "DC"
dcfcovid = dcfcovid.drop("Neighborhood", axis = 1)
dcfcovid

dc_area_county_COVID1 = pd.concat([mdcovid, vdhcovid], axis=0)
dc_area_county_COVID = pd.concat([dc_area_county_COVID1, dcfcovid], axis=0)
dc_area_county_COVID.to_csv(r'Clean_All_DC_Covid_ZIP_CODE.csv', index = False, header=True)
dc_area_county_COVID.isnull().sum(axis=0).tolist()
dc_area_county_COVID